{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors and matrices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.1) **No. of linearly independent vectors in ${\\mathbb R^m}$**.  The maximum length $n$ of a list of linearly independent vectors in $\\mathbb R^m$ is bounded by $m$.  If $n > m$, then the list is linearly dependent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.2) **Complexity of checking independence**. Suppose $n \\leq m.$ What is the time complexity of showing n vectors in $\\mathbb R^m$ are linearly independent? i.e. solving for nonzero solutions to $\\mathbf A\\mathbf x = \\mathbf 0$. For instance, we have $\\mathcal{O}(\\frac{2}{3} mn^2)$ using Gaussian elimination assuming $\\mathcal{O}(1)$ arithmetic which is a naive assumption as careless implementation can easily create numbers that can be [exponentially large](https://cstheory.stackexchange.com/questions/3921/what-is-the-actual-time-complexity-of-gaussian-elimination)! In practice, the best way to compute the rank of $\\mathbf A$ is through its SVD. This is, for example, how `numpy.linalg.matrix_rank` is implemented."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.3) **Basis is non-unique.**\n",
    "  A choice of basis is non-unique but gives unique coordinates for each vector once the choice of basis is fixed. Some basis are better than others for a particular task, e.g. describing a dataset better. There are algorithms such as PCA & ICA that try to minimize some objective function.\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.4) **Orthogonal matrices are precisely the linear isometries of $\\mathbb R^n$.** A matrix $\\mathbf A \\in \\mathbb R^n$ is an isometry if $\\lVert \\mathbf A \\mathbf x\\rVert^2 = \\lVert \\mathbf x \\rVert^2$ for all $\\mathbf x \\in \\mathbb R^n$. Note that $\\lVert \\mathbf A \\mathbf x \\rVert^2 = \\mathbf x^\\top\\mathbf A^\\top \\mathbf A \\mathbf x$ and $\\lVert \\mathbf x \\rVert^2 = \\mathbf x^\\top \\mathbf x$. So orthogonal matrices are isometries. Conversely, if a matrix $\\mathbf A$ is an isometry, we can let $\\mathbf x = \\mathbf e_i - \\mathbf e_j$ to get $\\mathbf e_i^\\top (\\mathbf A^\\top \\mathbf A) \\mathbf e_j = (\\mathbf A^\\top \\mathbf A)_ {ij} = \\delta_ {ij}$ where $\\delta_{ij}$ is the Kronecker delta or $\\mathbf A^\\top\\mathbf A = \\mathbf I$. This tells us that length preserving matrices in $\\mathbb R^n$ are necessarily orthogonal. Orthogonal matrices in $\\mathbb R^2$ are either rotations or reflections &mdash; both of these are length preserving. The more surprising result is that these are the only length preserving matrices in $\\mathbb R^2$!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.5) **Orthogonal matrices as projections.** An orthogonal matrix $\\mathbf U$ is defined as a matrix with orthonormal vectors in its column. It follows $\\mathbf U^\\top \\mathbf U = \\mathbf I.$ Since $\\mathbf U$ is invertible, we can use uniqueness of inverse to get $\\mathbf U \\mathbf U^\\top = \\mathbf I.$ However, we can obtain this latter identity geometrically. Let $\\mathbf x$ be a vector, then $\\mathbf x = \\sum_i \\mathbf u_i \\mathbf u_i^\\top \\mathbf x.$ This is true by uniqueness of components in a basis. Thus, $\\mathbf U \\mathbf U^\\top \\mathbf x = \\mathbf x$ for any $\\mathbf x,$ or $\\mathbf U \\mathbf U^\\top  = \\mathbf I.$ \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.6) **Shifting a matrix away from degeneracy:**\n",
    "  $\\mathbf{A} + \\lambda \\mathbf{I} \\simeq \\mathbf{A}.$ \n",
    "  Geometric interpretation: inflate a matrix from a degenerate plane towards being a sphere. This is a form of regularization. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.7) **Calculating the Hermitian transpose in Python.** Let `A` be a numpy array. The following calculates the Hermitian transpose:\n",
    "    \n",
    "1. `np.conj(A).T`\n",
    "2. `np.conj(A.T)`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.8) Four ways of matrix multiplication: \n",
    "\n",
    "  - **Outer product perspective** <br> \n",
    "    ```\n",
    "    AB[i, j] = sum(k, A[i, k] B[k, j]) \n",
    "             = sum(k, outer(A[:, k], B[k, :])[i, j]\n",
    "    ```\n",
    "  - **Row perspective**: <br> \n",
    "    ```\n",
    "    AB[i, :] = sum(k, A[i, k] B[k, :]) \n",
    "    ```\n",
    "  - **Column perspective**: <br> \n",
    "    ```\n",
    "    AB[:, j] = sum(k, A[:, k] B[k, j]) \n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3 (main, Mar 26 2022, 10:14:30) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74fc4d732c045e64e902852b64fc2bd2168aa0e65c0f64395f775ce1c5c468e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
