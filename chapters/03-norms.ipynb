{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication and Norms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3.1) **Symmetric product of two symmetric matrices.** Suppose $\\mathbf S$ and $\\mathbf T$ are symmetric matrices. What is the condition so that their product $\\mathbf S \\mathbf T$ is symmetric, i.e. $(\\mathbf S \\mathbf T)^\\top = \\mathbf S \\mathbf T$? Observe that $(\\mathbf S \\mathbf T)^\\top = \\mathbf T ^\\top \\mathbf S ^\\top = \\mathbf T \\mathbf S.$ Thus, the product of two symmetric matrices is symmetric if and only if the matrices commute. This works for a very small class of matrices, e.g. zeros or constant diagonal matrices. \n",
    "In the case of $2 \\times 2$ matrices, this is satisfied most naturally by  matrices with constant diagonal entries &mdash; a quirk that does not generalize to higher dimensions.\n",
    "The lack of symmetry turns out to be extremely important in machine-learning, multivariate statistics, and signal processing, and is a core part of the reason why linear classifiers are so successful [[Lec 56, Q&A]](https://www.udemy.com/course/linear-algebra-theory-and-implementation/learn/lecture/10738628#questions/13889570/): \n",
    "\n",
    ">  \"The lack of symmetry means that $\\mathbf C=\\mathbf B^{-1} \\mathbf A$ is not symmetric, which means that $\\mathbf C$ has non-orthogonal eigenvectors. In stats/data science/ML, most linear classifiers work by using generalized eigendecomposition on two data covariance matrices $\\mathbf B$ and $\\mathbf A$, and the lack of symmetry in $\\mathbf C$ turns a compression problem into a separation problem.\"\n",
    "\n",
    "(?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3.2) **Hadamard and standard multiplications are equivalent for diagonal matrices.** This can have consequences in practice. The following code in IPython shows that Hadamard multiplication is 3 times faster than standard multiplication in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639 ns ± 5.13 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "D = np.diag([1, 2, 3, 4, 5])\n",
    "\n",
    "%timeit D @ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 ns ± 2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit D * D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3.3) **Frobenius inner product and its induced norm.** The **Frobenius inner product** between two $m \\times n$ matrices $\\mathbf A$ and $\\mathbf B$ is defined as \n",
    "  $\n",
    "  \\langle \\mathbf A, \\mathbf B\\rangle_F \n",
    "  = \\sum_{i=1}^m \\sum_{j=1}^n a_{ij} b_{ij}. \n",
    "  $ \n",
    "  Two alternative ways of computing this: (1) reshape $\\mathbf A$ and $\\mathbf B$ as vectors, then take the dot product; and (2) $\\langle \\mathbf A, \\mathbf B\\rangle_F = \\text{tr}(\\mathbf A^\\top \\mathbf B)$ which is nice in theory, but makes *a lot* of unnecessary computation! The **Frobenius norm** is defined as\n",
    "  $$\n",
    "  \\lVert \\mathbf A \\rVert_F = \\sqrt{\\langle \\mathbf A, \\mathbf A\\rangle_F} = \\sqrt{\\text{tr} (\\mathbf A^\\top \\mathbf A)} = \\sqrt{\\sum\\sum {a_{ij}}^2}.\n",
    "  $$ \n",
    "  \n",
    "  The fastest way to calculate this in NumPy is the straightforward `(A * B).sum()`. Other ways of calculating (shown in the video) are slower: (1) `np.dot(A.reshape(-1, order='F'), B.reshape(-1, order='F'))` where `order='F'` means Fortran-like indexing or along the columns, and (2) `np.trace(A @ B)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.77 µs ± 6.64 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(2, 2)\n",
    "B = np.random.randn(2, 2)\n",
    "\n",
    "%timeit np.dot(A.reshape(-1, order='F'), B.reshape(-1, order='F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.01 µs ± 8.79 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.trace(A.T @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04 µs ± 10.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (A * B).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.** The Frobenius inner product is an inner product on $\\mathbb R^{m \\times n}$ in the same way that the usual dot product is an inner product on $\\mathbb R^{mn}$. It follows that the Frobenius norm $\\lVert \\cdot \\rVert_F$ is a norm as it is induced by the inner product $\\langle \\cdot, \\cdot \\rangle_F$ [[Prop. 6]](https://ai.stanford.edu/~gwthomas/notes/norms-inner-products.pdf). As usual for complex matrices we replace the transpose with the conjugate transpose: $\\langle \\mathbf A, \\mathbf B\\rangle_F =\\text{tr}(\\mathbf A^* \\mathbf B)$ and $\\lVert \\mathbf A \\rVert_F= \\sqrt{\\text{tr} (\\mathbf A^* \\mathbf A)} = \\sqrt{\\sum\\sum |a_{ij}|^2}.$ These are an inner product and a norm on $\\mathbb C^{m \\times n}$, as in the real case.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3.4) **Other norms.** The **operator norm** is defined as \n",
    "$$\\lVert \\mathbf A \\rVert = \\sup_{\\mathbf x \\neq \\mathbf 0} \\frac{\\lVert \\mathbf A \\mathbf x \\rVert_2}{\\lVert \\mathbf x \\rVert_2} = \\sup_{\\lVert\\mathbf x\\rVert_2 = 1} {\\lVert \\mathbf A \\mathbf x \\rVert_2} = \\sigma_1.$$ \n",
    "\n",
    "It follows that $\\lVert \\mathbf A \\mathbf x\\rVert_2 \\leq \\lVert \\mathbf A \\rVert \\lVert \\mathbf x\\rVert_2$. Another matrix norm is the **Schatten $p$-norm** defined as \n",
    "$$\\lVert \\mathbf A  \\rVert_p = \\left( \\sum_{i=1}^r \\sigma_i^p \\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "where $\\sigma_1, \\ldots, \\sigma_r$ are the singular values of $\\mathbf A$. That is, the Schatten $p$-norm is the $p$-norm of the vector of singular values of $\\mathbf A$. Recall that the singular values are the length of the axes of the ellipse, so the Schatten $p$-norm is a cumulative measure of how much $\\mathbf A$ expands the space in each dimension."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3.5) **Operator norm and singular values.** Note that $\\lVert \\mathbf A \\rVert_2 = \\sup_{\\lVert \\mathbf x \\rVert = 1} \\lVert \\mathbf A \\mathbf x \\rVert_2$ for the operator norm. Recall that the unit circle is transformed $\\mathbf A$ to an ellipse whose axes have length corresponding to the singular values of $\\mathbf A$. Geometrically, we can guess that $\\lVert \\mathbf A \\rVert_2 = \\sigma_1$ with $\\sigma_1$ being the largest singular value of $\\mathbf A$. Indeed, we verified this in `4_compute_svd.py` where it was shown that `σ₁ - max ‖Ax‖ / ‖x‖ = 1.67e-07`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bf457f4026a6353447ea08cc5de431bf2d4a57657f54daac3cf4f903fa850ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
