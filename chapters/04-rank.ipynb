{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank and Dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.1) **Rank as dimensionality of information.** The rank of $\\mathbf A \\in \\mathbb R^{m \\times n}$ is the maximal number of linearly independent columns of $\\mathbf A.$ It follows that $0 \\leq r \\leq \\min(m, n).$ Matrix rank has several applications, e.g. $\\mathbf A^{-1}$ exists for a square matrix whenever it has maximal rank. In applied settings, rank is used in PCA, Factor Analysis, etc. because rank is used to determine how much nonredundant information is contained in $\\mathbf A.$ \n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.2) **Computing the rank.** How to count the maximal number of linearly independent columns? (1) Row reduction (can be numerically unstable). (2) Best way is to use SVD. The rank of $\\mathbf A$ is the number $r$ of nonzero singular values of $\\mathbf A.$ This is how it's implemented in MATLAB and NumPy. The SVD is also used for rank estimation. Another way is to count the number of nonzero eigenvalues of $\\mathbf A$ provided $\\mathbf A$ has an eigendecomposition. Since this would imply that $\\mathbf A$ is similar to its matrix of eigenvalues. This is in general not true. Instead, we can count the eigenvalues of $\\mathbf A^\\top \\mathbf A$ or $\\mathbf A\\mathbf A^\\top$ &mdash; whichever is smaller &mdash; since an eigendecomposition for both matrices always exist. In practice, counting the rank requires setting a threshold below which values which determine rank are treated as zero, e.g. singular values $\\sigma_k$ in the SVD. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.3) **Rank can be difficult to calculate numerically.** For instance if we obtain $\\sigma_k = 10^{-13}$ numerically, is it a real nonzero singular value, or is it zero? In practice, we set thresholds. The choice of threshold can be arbitrary or domain specific, and in general, introduces its own difficulties. Another issue is noise, adding $\\epsilon\\mathbf I$ makes $\\mathbf A = [[1, 1], [1, 1]]$ rank two.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.4) **Finding a basis for the column space.** We will be particularly interested in finding a subset of the columns of $\\mathbf A$ that is a basis for $\\mathsf{C}(\\mathbf A).$ The problem in abstract terms is to find a linearly independent subset of a spanning set that spans the space. One can proceed iteratively. Let $\\mathbf a_1, \\ldots, \\mathbf a_n$ be the columns of $\\mathbf A$, take the largest $j$ such that $\\sum_{j=1}^n c_j \\mathbf a_j = \\mathbf 0$ and $c_j \\neq 0.$ We can remove this vector $\\mathbf a_j$ such that the remaining columns still span $\\mathsf{C}(\\mathbf A).$ Repeat until we get $r$ columns that is linearly independent, i.e. $\\sum_{j=1}^n c_j \\mathbf a_j = \\mathbf 0$ implies $c_j = 0$ for all $j.$ Further removing any vector fails to span the column space, or the column space is zero in the worst case, so we know this algorithm terminates. <br><br>\n",
    "Another way to construct a basis for $\\mathsf{C}(\\mathbf A)$ is to perform Gaussian elimination along the columns of $\\mathbf A$ as each step preserves the column space. The resulting pivot columns form a basis for $\\mathsf{C}(\\mathbf A)$ but is not a subset of the columns of $\\mathbf A.$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.5) **Basis and dimension.** Basis vectors are linearly independent vectors that span the vector space. We will be interested in finite-dimensional vector spaces, i.e. spaces that are spanned by finitely many vectors. By the above algorithm, we can always reduce the spanning set to a basis, so that a finite-dimensional vector space always has a (finite) basis. We know that a basis is not unique &mdash; in the previous bullet we constructed two bases for $\\mathsf{C}(\\mathbf A).$ However, once a basis $\\mathbf v_1, \\ldots, \\mathbf v_n$ is fixed, every vector $\\mathbf x$ has a unique representation $(x_1, \\ldots, x_n)$ such that $\\mathbf x = \\sum_{i=1}^n x_i \\mathbf v_i.$ We can think of this as a **parametrization** of the space by $n$ numbers. It is natural to ask whether there exists a more compressed parametrization, i.e. a basis of shorter length. It turns out that the length of any basis of a finite-dimensional vector space have the same length. Thus, we can think of this number as a property of the space which we define to be its **dimension**. This can be proved with the help of the ff. lemma since a basis is simultaneously spanning and linearly independent:\n",
    "\n",
    "> (Finite-dim.) The cardinality of any linearly independent set of vectors is bounded by the cardinality of any spanning set of the vector space.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Proof.**   The idea for the proof is that we can iteratively exchange vectors in a spanning set with vectors in linearly independent set while preserving the span. \n",
    "Let $\\mathbf u_1, \\ldots, \\mathbf u_{s}$ be a linearly independent list and $\\mathbf v_1, \\ldots, \\mathbf v_{t}$ be a spanning set in $V.$ We iteratively update the spanning set keeping it at fixed length $t.$ Append $\\mathbf u_1, \\mathbf v_1, \\ldots, \\mathbf v_{t}.$ This list is linearly dependent since $\\mathbf u_1 \\in V.$ Possibly reindexing, let $\\mathbf u_1$ depend on $\\mathbf v_1$, then $\\mathbf u_1, \\mathbf v_2, \\ldots, \\mathbf v_{t}$ spans $V.$ Now append $\\mathbf u_2, \\mathbf u_1, \\mathbf v_2, \\ldots, \\mathbf v_{t}.$ Note that $\\mathbf u_2$ cannot depend solely on $\\mathbf u_1$ by linear independence, so that $\\mathbf u_2$ depends on $\\mathbf v_2$ possibly reindexing. That is, we know $c_2 \\neq 0$ in the ff. equation:\n",
    "$$\n",
    "\\mathbf u_2 = c_1 \\mathbf u_1 + \\sum_{j=2}^t c_j \\mathbf v_j \\implies \\mathbf v_2 = \\frac{c_1}{c_2} \\mathbf u_1 - \\frac{1}{c_2} \\mathbf u_2 + \\sum_{j=3}^t \\frac{c_j}{c_2} \\mathbf v_j \n",
    "$$\n",
    "so that $\\mathbf u_2, \\mathbf u_1, \\mathbf v_3, \\ldots, \\mathbf v_{t}$ spans $V.$ That is, we have exchanged $\\mathbf u_2$ with $\\mathbf v_2$ in the spanning set. Repeat this until we get all $\\mathbf u_j$'s on the left end of the list. This necessarily implies that $s \\leq t$ since we cannot run out of $\\mathbf v_i$ vectors in the spanning set due to linear independence of the $\\mathbf u_j$'s. $\\square$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.6) **Obtaining a basis by counting.** This example shows how we can use the bound to reason about linearly independent lists/sets. A linearly independent set of length $r$ where $r$ is the dimension of the space is necessarily a basis. Otherwise, we can append the vector that is not spanned to get a linearly independent list of size $r+1.$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4.7) **Row rank = column rank.** \n",
    "Consider a step in Gaussian elimination along the rows of $\\mathbf A$ resulting in $\\tilde {\\mathbf A}.$ We know that $\\mathsf{R}(\\mathbf A) = \\mathsf{R}(\\tilde{\\mathbf A}).$ So its clear that the row rank remains unchanged.\n",
    "On the other hand, let's consider the independence of the columns after a step. We know there are $r$ basis vectors in the columns of $\\mathbf A$ and the $n - r$ non-basis vectors are a linear combination of the $r$ basis vectors. \n",
    "WLOG, suppose the first $r$ columns of $\\mathbf A$ form the basis for $\\mathsf{C}(\\mathbf A).$ Then, for $j > r$, there exist a vector $\\mathbf x$ such that $\\mathbf A \\mathbf x = \\mathbf 0$ and $x_j = -1$ which encode the dependencies. Moreover, the only solution of the homogeneous system such that $x_j = 0$ for $j > r$ is $\\mathbf x = \\mathbf 0.$ Observe that $\\mathbf A$ and $\\tilde{\\mathbf A}$ have the same null space as an inductive invariant, so that the index of the basis vectors in the columns of $\\mathbf A$ are carried over to $\\tilde{\\mathbf A}.$ It follows that, the column rank of $\\tilde{\\mathbf A}$ is equal to the column rank of $\\mathbf A.$ Thus, in every step of Gaussian elimination the column and row ranks are invariant. At the end of the algorithm, with $r$ pivots remaining, we can read off that $r$ maximally independent rows and $r$ maximally independent columns. It follows that the column and row ranks of $\\mathbf A$ are equal.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3 (main, Mar 26 2022, 10:14:30) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74fc4d732c045e64e902852b64fc2bd2168aa0e65c0f64395f775ce1c5c468e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
